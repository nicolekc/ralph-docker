{
  "name": "Investigations",
  "description": "Non-code deliverables that need human review before influencing framework design. All tasks produce documents with findings and recommendations. These can run in parallel — no dependencies between them. Human checkpoint: Review all findings before they shape the framework.",
  "signoff": "full",
  "tasks": [
    {
      "id": "001",
      "description": "Investigation: AGENTS.md pattern — right balance for per-directory knowledge. Research how different ralph/claude implementations handle AGENTS.md (per-directory hint files). Some are too liberal (write noise after every change), some too conservative (never write anything). Investigate: what triggers a write? what content is useful vs noise? how to prevent staleness? Produce findings in ralph-context/designs/ with a recommendation. NON-CODE DELIVERABLE — needs human review.",
      "outcome": "A findings document with concrete recommendations for when and what to write to AGENTS.md files",
      "verification": "Document exists, covers the noise-vs-knowledge tradeoff, includes a concrete recommendation",
      "dependencies": [],
      "status": "pending"
    },
    {
      "id": "002",
      "description": "Investigation: principle adherence hardening. The design principles are written but there's a risk new sessions won't feel their weight (see ralph-context/knowledge/principle-adherence-risks.md). Investigate: can we test for principle adherence? Can we increase principle 'weight' without becoming prescriptive? Is there an experiment we can run (e.g., give a deliberately over-specified task and see if the AI pushes back)? Produce findings with specific recommendations. NON-CODE DELIVERABLE — needs human review.",
      "outcome": "Concrete recommendations for ensuring principle adherence — through testing, review checklists, seed improvements, or other means",
      "verification": "Document exists with actionable recommendations. At least one is implementable in a subsequent task.",
      "dependencies": [],
      "status": "pending"
    },
    {
      "id": "003",
      "description": "Investigation: human inbox mechanism. When ralph produces non-code deliverables (investigations, designs, architecture) that need human review, how does the human know where to look and what to review? Current 'needs_human_review' status is necessary but not sufficient. Research approaches: summary file, specific directory, PRD status integration. NON-CODE DELIVERABLE — needs human review.",
      "outcome": "A concrete design for how non-code deliverables surface to the human",
      "verification": "Document exists with a recommended approach. Simple enough to implement in one follow-up task.",
      "dependencies": [],
      "status": "pending"
    },
    {
      "id": "004",
      "description": "Investigation: parallelization principles. When ralph has independent tasks, it can dispatch them in parallel. But what are the right guardrails? When is parallelization safe vs risky? How do file conflicts get detected? What about shared test infrastructure? Research approaches and produce recommendations. This should be informed by real experience — if PRD 002 has been completed, reference what worked and what didn't. NON-CODE DELIVERABLE — needs human review.",
      "outcome": "Guidelines for when and how to parallelize task execution, with concrete guardrails",
      "verification": "Document exists with actionable guidelines. Covers at least: file conflict detection, shared resource handling, and when NOT to parallelize.",
      "dependencies": [],
      "status": "pending"
    },
    {
      "id": "005",
      "description": "Investigation: how should not-ready tasks be represented? Current approach is a 'draft' status on tasks. But alternatives exist: separate directory for future tasks, a separate 'backlog' PRD, a naming convention (prefix with _), or something else. The design should balance: (1) signaling 'not ready' clearly, (2) not adding unnecessary framework machinery, (3) being inspectable by humans. See ralph-context/knowledge/draft-tasks-pattern.md for initial thinking (not settled). NON-CODE DELIVERABLE — needs human review.",
      "outcome": "A recommendation for how to represent not-ready tasks, with rationale for the chosen approach",
      "verification": "Document exists with at least 3 options considered and a clear recommendation with tradeoffs.",
      "dependencies": [],
      "status": "pending"
    }
  ]
}
